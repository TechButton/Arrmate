# Arrmate — docker-compose.yml
#
# BUILD FROM SOURCE (development / self-hosted build)
# For the pre-built Docker Hub image, use docker-compose.prod.yml instead.
#
# USAGE:
#   cp .env.example .env && nano .env
#   docker compose up -d
#
# GPU ACCELERATION FOR OLLAMA:
#   Set COMPOSE_FILE in your .env to merge the right override:
#     NVIDIA: COMPOSE_FILE=docker-compose.yml:docker-compose.ollama-nvidia.yml
#     AMD:    COMPOSE_FILE=docker-compose.yml:docker-compose.ollama-amd.yml
#
# EXTERNAL OLLAMA:
#   If Ollama runs on a different machine, set OLLAMA_BASE_URL in .env
#   to that machine's address (e.g. http://192.168.1.x:11434) and
#   comment out the ollama service below to avoid a duplicate instance.

services:

  # ─────────────────────────────────────────────
  # Arrmate (build from source)
  # ─────────────────────────────────────────────
  arrmate:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: arrmate
    restart: unless-stopped
    ports:
      - "${API_PORT:-8000}:8000"
    environment:
      # Application
      - API_HOST=0.0.0.0
      - API_PORT=${API_PORT:-8000}
      - LOG_LEVEL=${LOG_LEVEL:-INFO}

      # LLM provider (ollama | openai | anthropic)
      - LLM_PROVIDER=${LLM_PROVIDER:-ollama}

      # Ollama
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-http://ollama:11434}
      - OLLAMA_MODEL=${OLLAMA_MODEL:-qwen2.5:7b}

      # OpenAI (leave blank to disable)
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - OPENAI_MODEL=${OPENAI_MODEL:-gpt-4o}

      # Anthropic (leave blank to disable)
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}
      - ANTHROPIC_MODEL=${ANTHROPIC_MODEL:-claude-3-5-sonnet-20241022}

      # ── Media Services ──────────────────────────────────────────
      # Commented out in .env = empty string here = not shown in UI
      - SONARR_URL=${SONARR_URL:-}
      - SONARR_API_KEY=${SONARR_API_KEY:-}
      - RADARR_URL=${RADARR_URL:-}
      - RADARR_API_KEY=${RADARR_API_KEY:-}
      - LIDARR_URL=${LIDARR_URL:-}
      - LIDARR_API_KEY=${LIDARR_API_KEY:-}
      - WHISPARR_URL=${WHISPARR_URL:-}
      - WHISPARR_API_KEY=${WHISPARR_API_KEY:-}
      - BAZARR_URL=${BAZARR_URL:-}
      - BAZARR_API_KEY=${BAZARR_API_KEY:-}
      - AUDIOBOOKSHELF_URL=${AUDIOBOOKSHELF_URL:-}
      - AUDIOBOOKSHELF_API_KEY=${AUDIOBOOKSHELF_API_KEY:-}
      - LAZYLIBRARIAN_URL=${LAZYLIBRARIAN_URL:-}
      - LAZYLIBRARIAN_API_KEY=${LAZYLIBRARIAN_API_KEY:-}
      - READARR_URL=${READARR_URL:-}
      - READARR_API_KEY=${READARR_API_KEY:-}
      - HUNTARR_URL=${HUNTARR_URL:-}
      - HUNTARR_API_KEY=${HUNTARR_API_KEY:-}
      - PLEX_URL=${PLEX_URL:-}
      - PLEX_TOKEN=${PLEX_TOKEN:-}
    networks:
      - arrmate-net
      # If using Traefik, also add the Traefik network:
      # - traefik
    depends_on:
      ollama:
        condition: service_started
    healthcheck:
      test: ["CMD", "python", "-c", "import httpx; httpx.get('http://localhost:8000/health')"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    # ── Traefik labels (uncomment if using Traefik) ──────────────
    # Also set TRAEFIK_DOMAIN, TRAEFIK_ENTRYPOINT, TRAEFIK_CERTRESOLVER,
    # and TRAEFIK_NETWORK in your .env
    # labels:
    #   - "traefik.enable=true"
    #   - "traefik.http.routers.arrmate.rule=Host(`${TRAEFIK_DOMAIN}`)"
    #   - "traefik.http.routers.arrmate.entrypoints=${TRAEFIK_ENTRYPOINT:-websecure}"
    #   - "traefik.http.routers.arrmate.tls.certresolver=${TRAEFIK_CERTRESOLVER:-letsencrypt}"
    #   - "traefik.http.services.arrmate.loadbalancer.server.port=8000"
    #   - "traefik.docker.network=${TRAEFIK_NETWORK:-traefik}"

  # ─────────────────────────────────────────────
  # Ollama — Local LLM
  # ─────────────────────────────────────────────
  # Comment this entire service out if you are using:
  #   - An external Ollama on another machine
  #   - OpenAI or Anthropic as your LLM provider
  #
  # GPU acceleration is handled by override files:
  #   NVIDIA: docker-compose.ollama-nvidia.yml
  #   AMD:    docker-compose.ollama-amd.yml
  # Set COMPOSE_FILE in .env to include the override.
  ollama:
    image: ollama/ollama:latest
    container_name: arrmate-ollama
    restart: unless-stopped
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
    networks:
      - arrmate-net
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s


# ─────────────────────────────────────────────────────────────────
# Networks
# ─────────────────────────────────────────────────────────────────
networks:
  arrmate-net:
    name: arrmate-net
  # Traefik network — only needed if TRAEFIK_NETWORK is set.
  # Must match the network your Traefik instance is on.
  # traefik:
  #   external: true
  #   name: ${TRAEFIK_NETWORK:-traefik}


# ─────────────────────────────────────────────────────────────────
# Volumes
# ─────────────────────────────────────────────────────────────────
volumes:
  ollama-data:
    driver: local
