# ===================================================================
# ARRMATE CONFIGURATION
# ===================================================================
#
# HOW THIS FILE WORKS:
#   - Copy this file to .env and edit it for your setup
#   - Lines starting with # are comments and are IGNORED
#   - A service that is commented out will NOT appear in the Arrmate UI
#   - Uncomment a service block (both URL and KEY) to enable it
#
# DOCKER HOSTNAME GUIDE:
#   If a service runs in Docker on the SAME host as Arrmate, use the
#   Docker service/container name as the hostname — NOT localhost:
#
#     Same Docker host:  http://sonarr:8989       ← use this
#     External host:     http://192.168.1.50:8989  ← use this
#     On this machine
#     but NOT Docker:    http://host.docker.internal:8989
#
#   Using localhost inside a container refers to the container itself,
#   not your host machine. This is the most common connection mistake.
#
# ===================================================================


# ===================================================================
# DOCKER COMPOSE FILE SELECTION
# ===================================================================
# Docker Compose reads COMPOSE_FILE to know which files to merge.
# Select your Ollama runtime by uncommenting ONE option below.
#
# If you are NOT using Ollama (using OpenAI or Anthropic instead),
# you can omit Ollama from your compose entirely — see the Ollama
# section below.

# CPU only (default — works everywhere, no GPU required):
COMPOSE_FILE=docker-compose.yml

# NVIDIA GPU acceleration for Ollama (requires nvidia-container-toolkit):
# COMPOSE_FILE=docker-compose.yml:docker-compose.ollama-nvidia.yml

# AMD GPU acceleration for Ollama (requires ROCm drivers):
# COMPOSE_FILE=docker-compose.yml:docker-compose.ollama-amd.yml


# ===================================================================
# LLM PROVIDER
# ===================================================================
# Arrmate uses tool/function calling to parse natural language commands.
# Choose ONE provider. The others can remain commented out.
#
# Options: ollama  (local, free)
#          openai  (cloud, paid)
#          anthropic (cloud, paid)

LLM_PROVIDER=ollama


# -------------------------------------------------------------------
# Option A: Ollama — Local LLM (free, runs on your hardware)
# -------------------------------------------------------------------
# Ollama location options:
#
#   In Docker on this host:       http://ollama:11434       (default)
#   External machine on LAN:      http://192.168.1.x:11434
#   On this machine, not Docker:  http://host.docker.internal:11434
#
# If using an EXTERNAL Ollama instance, set the URL above and comment
# out the 'ollama' service in docker-compose.yml to avoid running a
# second local instance.
#
# RECOMMENDED MODELS (must support tool/function calling):
#   qwen2.5:7b       ~5GB VRAM  Best balance of speed + accuracy  ← DEFAULT
#   llama3.1:8b      ~5GB VRAM  Native tool calling, well tested
#   mistral-nemo:12b ~8GB VRAM  Good reasoning, reliable tool calling
#   qwen2.5:14b      ~10GB VRAM Better accuracy for complex commands
#   llama3.3:70b     ~48GB VRAM Best quality, high hardware requirement
#
# AVOID: llama3.2:3b (too small), phi3, older mistral:7b (unreliable)
#
# Pull your chosen model after starting:
#   docker compose exec ollama ollama pull qwen2.5:7b

OLLAMA_BASE_URL=http://ollama:11434
OLLAMA_MODEL=qwen2.5:7b


# -------------------------------------------------------------------
# Option B: OpenAI (commented out = disabled)
# -------------------------------------------------------------------
# Uncomment both lines and set LLM_PROVIDER=openai above to use OpenAI.
# Models with reliable tool calling: gpt-4o, gpt-4-turbo, gpt-4

# OPENAI_API_KEY=sk-your-key-here
# OPENAI_MODEL=gpt-4o


# -------------------------------------------------------------------
# Option C: Anthropic / Claude (commented out = disabled)
# -------------------------------------------------------------------
# Uncomment both lines and set LLM_PROVIDER=anthropic above to use Claude.

# ANTHROPIC_API_KEY=sk-ant-your-key-here
# ANTHROPIC_MODEL=claude-3-5-sonnet-20241022


# ===================================================================
# MEDIA SERVICES
# ===================================================================
#
# IMPORTANT:
#   Commented out = the service will NOT appear in the Arrmate UI at all.
#   Uncomment both the URL and KEY lines for a service to enable it.
#
#   Same Docker host → use container/service name (e.g., http://sonarr:8989)
#   External host    → use IP or hostname   (e.g., http://192.168.1.50:8989)


# -------------------------------------------------------------------
# Sonarr (TV Shows) — FULLY SUPPORTED
# -------------------------------------------------------------------
# Get your API key: Sonarr → Settings → General → Security → API Key

SONARR_URL=http://sonarr:8989
SONARR_API_KEY=your-sonarr-api-key


# -------------------------------------------------------------------
# Radarr (Movies) — FULLY SUPPORTED
# -------------------------------------------------------------------
# Get your API key: Radarr → Settings → General → Security → API Key

RADARR_URL=http://radarr:7878
RADARR_API_KEY=your-radarr-api-key


# -------------------------------------------------------------------
# Lidarr (Music) — TESTING REQUIRED
# -------------------------------------------------------------------
# Get your API key: Lidarr → Settings → General → Security → API Key

# LIDARR_URL=http://lidarr:8686
# LIDARR_API_KEY=your-lidarr-api-key


# -------------------------------------------------------------------
# Whisparr (Adult Content) — TESTING REQUIRED
# -------------------------------------------------------------------
# Get your API key: Whisparr → Settings → General → Security → API Key

# WHISPARR_URL=http://whisparr:6969
# WHISPARR_API_KEY=your-whisparr-api-key


# -------------------------------------------------------------------
# Bazarr (Subtitle Management) — TESTING REQUIRED
# Requires Sonarr and/or Radarr to already be configured above.
# -------------------------------------------------------------------
# Get your API key: Bazarr → Settings → General → Security → API Key

# BAZARR_URL=http://bazarr:6767
# BAZARR_API_KEY=your-bazarr-api-key


# -------------------------------------------------------------------
# AudioBookshelf (Audiobooks & Podcasts) — TESTING REQUIRED
# -------------------------------------------------------------------
# Get your API token: AudioBookshelf → Settings → Security → API Tokens

# AUDIOBOOKSHELF_URL=http://audiobookshelf:13378
# AUDIOBOOKSHELF_API_KEY=your-audiobookshelf-api-token


# -------------------------------------------------------------------
# LazyLibrarian (Books & Audiobooks with Downloading) — TESTING REQUIRED
# -------------------------------------------------------------------
# Get your API key: LazyLibrarian → Config → Web Interface → API Key

# LAZYLIBRARIAN_URL=http://lazylibrarian:5299
# LAZYLIBRARIAN_API_KEY=your-lazylibrarian-api-key


# -------------------------------------------------------------------
# Readarr (Books/Audiobooks) — DEPRECATED (Project Retired 2026)
# Consider alternatives: AudioBookshelf, LazyLibrarian
# -------------------------------------------------------------------

# READARR_URL=http://readarr:8787
# READARR_API_KEY=your-readarr-api-key


# -------------------------------------------------------------------
# huntarr.io (Orchestration & Automation) — TESTING REQUIRED
# -------------------------------------------------------------------
# Get your API key: huntarr.io → Settings → General → API Key

# HUNTARR_URL=http://huntarr:3000
# HUNTARR_API_KEY=your-huntarr-api-key


# -------------------------------------------------------------------
# Plex Media Server — TESTING REQUIRED
# Uses a token, not an API key. Find yours:
#   app.plex.tv → open any item → "..." → Get Info → View XML
#   The X-Plex-Token value appears in the URL bar.
# -------------------------------------------------------------------

# PLEX_URL=http://plex:32400
# PLEX_TOKEN=your-plex-token-here


# ===================================================================
# TRAEFIK REVERSE PROXY (optional)
# ===================================================================
# If you use Traefik as your reverse proxy, uncomment the variables
# below and also uncomment the Traefik labels section in
# docker-compose.yml / docker-compose.prod.yml.
#
# Traefik label reference (set on the arrmate service):
#   traefik.enable=true
#   traefik.http.routers.arrmate.rule=Host(`arrmate.yourdomain.com`)
#   traefik.http.routers.arrmate.entrypoints=websecure
#   traefik.http.routers.arrmate.tls.certresolver=letsencrypt
#   traefik.http.services.arrmate.loadbalancer.server.port=8000
#
# Your Traefik container must be on the same Docker network as Arrmate.
# Set TRAEFIK_NETWORK to match the network Traefik is attached to.

# TRAEFIK_DOMAIN=arrmate.yourdomain.com
# TRAEFIK_ENTRYPOINT=websecure
# TRAEFIK_CERTRESOLVER=letsencrypt
# TRAEFIK_NETWORK=traefik


# ===================================================================
# AUTHENTICATION (optional)
# ===================================================================
# Arrmate's web UI and API are open by default. To enable login
# protection, go to Settings in the web UI and create credentials.
#
# SECRET_KEY is used to sign session cookies. If left empty, one is
# auto-generated at startup — but sessions will be lost on restart.
# Set a fixed value for persistent sessions across container restarts.
#
# AUTH_DATA_DIR is where auth.json (credentials) is stored. In Docker,
# this should be on a persistent volume (the compose files mount
# arrmate-data:/data by default).

# SECRET_KEY=your-random-secret-key-here
# AUTH_DATA_DIR=/data


# ===================================================================
# ARRMATE APPLICATION SETTINGS
# ===================================================================

API_HOST=0.0.0.0
API_PORT=8000
LOG_LEVEL=INFO

# Timezone (used by media service containers)
TZ=America/New_York
